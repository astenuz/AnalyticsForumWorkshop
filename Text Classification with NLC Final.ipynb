{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## Importar el dataset de noticias en espa\u00f1ol", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install spacy\n#!python -m spacy download es\n#!pip install stop_words\n#!pip install dplython\n#!pip install stop_words\n#!pip install install --upgrade scikit-learn"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/sparse/lil.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import _csparsetools\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/sparse/lil.py:16: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from . import _csparsetools\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/sklearn/utils/__init__.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .murmurhash import murmurhash3_32\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/linalg/basic.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._solve_toeplitz import levinson\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/linalg/basic.py:21: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._solve_toeplitz import levinson\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/linalg/__init__.py:190: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._decomp_update import *\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/linalg/__init__.py:190: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._decomp_update import *\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/special/__init__.py:627: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._ufuncs import *\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/special/__init__.py:627: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._ufuncs import *\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._group_columns import group_dense, group_sparse\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:8: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._group_columns import group_dense, group_sparse\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import vonmises_cython\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:24: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from . import vonmises_cython\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/stats/stats.py:188: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._rank import tiecorrect\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/stats/stats.py:188: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from ._rank import tiecorrect\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._logistic_sigmoid import _log_logistic_sigmoid\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .expected_mutual_info_fast import expected_mutual_information\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/spatial/__init__.py:92: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .ckdtree import *\n/usr/local/src/bluemix_jupyter_bundle.v83/notebook/lib/python2.7/site-packages/scipy/spatial/__init__.py:92: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from .ckdtree import *\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
                }
            ], 
            "source": "from io import BytesIO \nimport requests \nimport sys\nimport types\nimport pandas as pd\nimport numpy as np\nimport spacy\nfrom stop_words import get_stop_words\nimport string\nimport json\nfrom dplython import (DplyFrame, X, diamonds, select, sift, sample_n,\n    sample_frac, head, arrange, mutate, group_by, summarize, DelayFunction) \nimport sklearn\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.cross_validation import train_test_split"
        }, 
        {
            "source": "Conect\u00e9monos a una base de Hive, com\u00fanmente usada en Big Data, donde tenemos almacenadas las noticias", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Agregar el c\u00f3digo del paso _\"Agregar las credenciales para conectarse a la base de datos de Hive\"_", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- categoria: string (nullable = true)\n |-- url: string (nullable = true)\n |-- contenido: string (nullable = true)\n\n"
                }
            ], 
            "source": "from ingest.Connectors import Connectors\nfrom pyspark.sql import SQLContext\n\nsqlContext = SQLContext(sc)\n\nHiveloadOptions = { Connectors.Hive.HOST                        : hive_credentials[\"host\"],\n                      Connectors.Hive.PORT                      : hive_credentials[\"port\"],\n                      Connectors.Hive.DATABASE                  : hive_credentials[\"database\"],\n                      Connectors.Hive.USERNAME                  : hive_credentials[\"username\"],\n                      Connectors.Hive.PASSWORD                  : hive_credentials[\"password\"],\n                      Connectors.Hive.SOURCE_TABLE_NAME         : \"noticias_sr\"}\n\nhive_df = sqlContext.read.format(\"com.ibm.spark.discover\").options(**HiveloadOptions).load()\nhive_df.printSchema()\n#hive_df.show(10)"
        }, 
        {
            "source": "Definamos una funci\u00f3n para transformar los archivos de texto en un dataframe de Pandas", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "news_df = hive_df.toPandas()"
        }, 
        {
            "source": "## Preprocesemos los datos", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Revisemos el estado de los datos", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 6, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>contenido</th>\n    </tr>\n    <tr>\n      <th>categoria</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>deportes</th>\n      <td>1102</td>\n      <td>1102</td>\n    </tr>\n    <tr>\n      <th>economia</th>\n      <td>586</td>\n      <td>586</td>\n    </tr>\n    <tr>\n      <th>politica</th>\n      <td>237</td>\n      <td>237</td>\n    </tr>\n    <tr>\n      <th>tecnologia</th>\n      <td>437</td>\n      <td>437</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "             url  contenido\ncategoria                  \ndeportes    1102       1102\neconomia     586        586\npolitica     237        237\ntecnologia   437        437"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "news_df.groupby('categoria').count()"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>categoria</th>\n      <th>url</th>\n      <th>contenido</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2362</td>\n      <td>2362</td>\n      <td>2362</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>4</td>\n      <td>1908</td>\n      <td>1859</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>deportes</td>\n      <td>https://elpais.com/deportes/2018/02/25/actuali...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1102</td>\n      <td>8</td>\n      <td>330</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "       categoria                                                url contenido\ncount       2362                                               2362      2362\nunique         4                                               1908      1859\ntop     deportes  https://elpais.com/deportes/2018/02/25/actuali...          \nfreq        1102                                                  8       330"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "news_df.describe()"
        }, 
        {
            "source": "Como se ve en la anterior tabla, la cantidad de noticias no es unica. Quitemos las noticias que est\u00e9n duplicadas o que puedan estar vac\u00edas", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def preprocess(text_df,text_col_name):\n    #Antes de quitarlos, reemplacemos los textos sin contenido por \"nan\"'s y despu\u00e9s s\u00ed removemos los textos que tengan valor faltante\n    text_df = text_df.apply(lambda x: x.str.strip()).replace('', np.nan)\n    \n    #Remover las noticias sin links\n    text_df.dropna(subset=[text_col_name], inplace=True)\n    \n    #Remover las noticias duplicadas\n    text_df.drop_duplicates(subset=[text_col_name], inplace=True)\n    \n    #Resetear los \u00edndices\n    text_df = text_df.reset_index(drop=True)\n    \n    return(text_df)"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>categoria</th>\n      <th>url</th>\n      <th>contenido</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1857</td>\n      <td>1854</td>\n      <td>1857</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>4</td>\n      <td>1668</td>\n      <td>1857</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>deportes</td>\n      <td>https://elpais.com/tecnologia/2016/04/19/porta...</td>\n      <td>Banfield peg\u00f3 y aguant\u00f3 como un buen campe\u00f3n d...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>905</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "       categoria                                                url  \\\ncount       1857                                               1854   \nunique         4                                               1668   \ntop     deportes  https://elpais.com/tecnologia/2016/04/19/porta...   \nfreq         905                                                  6   \n\n                                                contenido  \ncount                                                1857  \nunique                                               1857  \ntop     Banfield peg\u00f3 y aguant\u00f3 como un buen campe\u00f3n d...  \nfreq                                                    1  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "news_df = preprocess(news_df,'contenido')\nnews_df.describe()"
        }, 
        {
            "source": "Para usar _Natural Language Classifier_ de _IBM_ es necesario que ninguno de los textos supere 1024 caracteres. Para evitar esto, podemos separar las noticias en parrafos de longitud fija o en oraciones usando el paquete spaCy. \n\nComo el API de _NLC_ est\u00e1 m\u00e1s orientada a an\u00e1lisis de texto en redes sociales o en conversaciones, separemos los textos a un nivel de oraciones", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Separar oraciones", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/spacy/language.py:15: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .tokenizer import Tokenizer\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/spacy/language.py:15: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from .tokenizer import Tokenizer\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/msgpack_numpy.py:179: PendingDeprecationWarning: encoding is deprecated, Use raw=False instead.\n  return _unpacker.unpack(stream, encoding=encoding, **kwargs)\n/gpfs/fs01/user/s3df-546c691ce14f7e-9e6e67bd3b24/.local/lib/python2.7/site-packages/msgpack_numpy.py:84: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n  dtype=np.dtype(descr)).reshape(obj[b'shape'])\n"
                }
            ], 
            "source": "#Definir el idioma a usar con spaCy\nnlp = spacy.load('es')\n\ndef separateSentences(text):\n\n    #Correr spaCy en el texto\n    documents = nlp(text)\n\n    #Separar en oraciones\n    sentences = [sent.string.strip() for sent in documents.sents]\n    \n    return sentences"
        }, 
        {
            "source": "Usemos la funci\u00f3n con un ejemplo", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[u'Esto, porque la que se hizo en el 2016, con todo y las cr\\xedticas que ha tenido, principalmente por haber ayudado a frenar el consumo de los hogares con el incremento del IVA, ser\\xeda suficiente para que el Gobierno pueda obtener los ingresos necesarios que le permitan mantener a raya el d\\xe9ficit fiscal.', u'Claro est\\xe1, paralelamente se tendr\\xeda que organizar el gasto p\\xfablico e impulsar el crecimiento de la econom\\xeda.']\n"
                }
            ], 
            "source": "text = 'Esto, porque la que se hizo en el 2016, con todo y las cr\u00edticas que ha tenido, principalmente por haber ayudado a frenar el consumo de los hogares con el incremento del IVA, ser\u00eda suficiente para que el Gobierno pueda obtener los ingresos necesarios que le permitan mantener a raya el d\u00e9ficit fiscal. Claro est\u00e1, paralelamente se tendr\u00eda que organizar el gasto p\u00fablico e impulsar el crecimiento de la econom\u00eda.'\noraciones = separateSentences(text.decode('utf-8'))\nprint(oraciones)"
        }, 
        {
            "source": "### Remover s\u00edmbolos de puntuaci\u00f3n dentro de cada oraci\u00f3n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Para poder pasarlo a _NLC_, es necesario remover algunos s\u00edmbolos para que el servicio de _IBM_ lo pueda procesar", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Definir el conjunto de s\u00edmbolos de puntuaci\u00f3n a remover\nSYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"\u201c\", \"\u201d\", \"'ve\",'\u00bf',\"\u00a1\"]\n\nWHITES = [\"\", \"\\n\", \"\\n\\n\"]\n\n#Lista negra de simbolos\nBLACKLIST = SYMBOLS + WHITES"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def remove_punctuation(s):\n    for char in BLACKLIST:\n        s = s.replace(char, '')\n    return(s)"
        }, 
        {
            "source": "Usemos la funci\u00f3n en una de nuestras oraciones anteriores", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 14, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'Esto porque la que se hizo en el 2016 con todo y las cr\\xc3\\xadticas que ha tenido principalmente por haber ayudado a frenar el consumo de los hogares con el incremento del IVA ser\\xc3\\xada suficiente para que el Gobierno pueda obtener los ingresos necesarios que le permitan mantener a raya el d\\xc3\\xa9ficit fiscal'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "remove_punctuation(oraciones[0].encode(encoding='UTF-8',errors='strict'))"
        }, 
        {
            "source": "### Apliquemos esto a todo el texto", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def separar_texto(text_df):\n    broken_text = []\n    for i in range(0,len(text_df['contenido'])):\n        try:\n            text = text_df['contenido'][i]\n            category = text_df['categoria'][i]\n            sentences = separateSentences(text)\n            \n            for sentence in sentences:\n                sentence = remove_punctuation(sentence.encode(encoding='UTF-8',errors='strict'))\n                record = (sentence, category)\n                broken_text.append(record)\n        except:\n            continue\n\n    broken_df = pd.DataFrame.from_records(broken_text)\n    \n    return(broken_df)"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "news_sentence_df = separar_texto(news_df)"
        }, 
        {
            "source": "Veamos como queda el dataframe despu\u00e9s de separarlo en oraciones", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 17, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>41329</td>\n      <td>41329</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>30068</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>La cantante Rosal\u00eda acompa\u00f1ada del productor E...</td>\n      <td>deportes</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>161</td>\n      <td>24544</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "                                                        0         1\ncount                                               41329     41329\nunique                                              30068         4\ntop     La cantante Rosal\u00eda acompa\u00f1ada del productor E...  deportes\nfreq                                                  161     24544"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "news_sentence_df.describe()"
        }, 
        {
            "source": "Algunas oraciones son iguales, seguramente por terminaciones como <b>\"ECONOM\u00cdA Y NEGOCIOS\"</b> en algunos art\u00edculos. Quitemos una vez m\u00e1s los duplicados", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "news_sentence_df = preprocess(news_sentence_df,0)"
        }, 
        {
            "source": "Algunas oraciones pueden tener m\u00e1s de 1024 caracteres, por lo que es mejor limitarlas en caso que existan. Empecemos por contar cuantas son", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 19, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "12"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "def count_over_limit(text_df):\n    count = 0\n    for txt in text_df:\n        if(len(txt) >1024):\n            count = count + 1\n    return(count) \n\ncount_over_limit(news_sentence_df[0])"
        }, 
        {
            "source": "Son muy pocas las noticias, por lo que podr\u00edamos quitarlas del dataset o limitarlas. Optemos por la segunda", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def limitar_texto(text_df):\n    max_char = 1024\n    for i in range(0,len(text_df[0])):\n        if(len(text_df.loc[i,0]) >1024):\n            text_df.loc[i,0] = text_df.loc[i,0][0:1024]\n        \n    return(text_df)"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 21, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "news_sentence_df = limitar_texto(news_sentence_df)\ncount_over_limit(news_sentence_df[0])"
        }, 
        {
            "source": "### Separemos nuestros datos en datos de entrenamiento y datos de validaci\u00f3n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "news_train, news_test, cat_train, cat_test = train_test_split(\n    news_sentence_df.drop(1, axis = 1), news_sentence_df[1], test_size=0.4, random_state=42)\n\n#Dado el formato que requiere NLC\ntrain = pd.concat([news_train, cat_train], axis=1)"
        }, 
        {
            "source": "Exportemos nuestra lista de entrenamiento a un archivo csv para pasarselo al API _NLC_", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train.to_csv('train.csv',header=False,index=False,encoding=\"utf-8\")"
        }, 
        {
            "source": "## Entrenemos el Clasificador", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Conect\u00e9monos con el servicio de _Natural Language Classifier_", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Instalar el sdk de Watson\n#!pip install --upgrade watson-developer-cloud\n#Importar a Python \nfrom watson_developer_cloud import NaturalLanguageClassifierV1"
        }, 
        {
            "source": "Agregar el c\u00f3digo del paso _Entrenar el clasificador Natural Language Classifier_", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Colocar aqui Credenciales NLC"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Definir las credenciales para poder usar el servicio\nnatural_language_classifier = NaturalLanguageClassifierV1(\n    username=nlc_username,\n    password=nlc_password\n)"
        }, 
        {
            "source": "Enviemos los datos de entrenamiento y creemos el clasificador. Dado que toma algunas horas en entrenar, para el _workshop_ ya hemos entrenado el clasificador", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open('../work/train.csv', 'rb') as training_data:\n    metadata=json.dumps({'name': 'Clasificador de noticias','language': 'es'})\n    classifier = natural_language_classifier.create_classifier(\n        metadata=metadata,\n        training_data=training_data\n    )"
        }, 
        {
            "source": "Este es el id del clasificador que ya hab\u00edamos entrenado antes", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Guardemos el id del clasificador para revisar su estado y pedirle clasificaciones\nclassifier_id = 'f7ea68x308-nlc-144'"
        }, 
        {
            "source": "Revisemos el estado del clasificador con el id que retorna el API. Las opciones son _training_ o _available_. En este caso ya est\u00e1 disponible", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{\n  \"status\": \"Available\", \n  \"name\": \"Clasificador de noticias\", \n  \"language\": \"es\", \n  \"created\": \"2018-03-06T21:19:20.203Z\", \n  \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/f7ea68x308-nlc-144\", \n  \"status_description\": \"The classifier instance is now available and is ready to take classifier requests.\", \n  \"classifier_id\": \"f7ea68x308-nlc-144\"\n}\n"
                }
            ], 
            "source": "status = natural_language_classifier.get_classifier(classifier_id)\nprint (json.dumps(status, indent=2))"
        }, 
        {
            "source": "{\n  \"status\": \"Training\", \n  \"name\": \"Clasificador de noticias\", \n  \"language\": \"es\", \n  \"created\": \"2018-03-06T02:38:16.160Z\", \n  \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/f7e8dex307-nlc-30\", \n  \"status_description\": \"The classifier instance is in its training phase, not yet ready to accept classify requests\", \n  \"classifier_id\": \"f7e8dex307-nlc-30\"\n}", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Una vez ya est\u00e9 entrenado, revisemos que tan bueno es clasificando noticias que estaban en los datos de entrenamiento", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{\n  \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/f7ea68x308-nlc-144\", \n  \"text\": \"En pleno debate electoral, cuando los candidatos a la presidencia se la juegan apostando por subir o no subir los impuestos, lo que a los t\\u00e9cnicos les parece populismo, el propio Fondo Monetario Internacional (FMI), que por dem\\u00e1s, sorprendi\\u00f3 con una baja en su proyecci\\u00f3n de crecimiento de la econom\\u00eda en el 2018 (de 3 a 2,7 %) y recomend\\u00f3 seguir recortando las tasas de inter\\u00e9s del Emisor, tambi\\u00e9n le dijo al ministro de Hacienda, Mauricio C\\u00e1rdenas, que no ve la necesidad de una nueva reforma tributaria en el pa\\u00eds.\", \n  \"classes\": [\n    {\n      \"class_name\": \"economia\", \n      \"confidence\": 0.9938446210868216\n    }, \n    {\n      \"class_name\": \"politica\", \n      \"confidence\": 0.0047897735231073445\n    }, \n    {\n      \"class_name\": \"deportes\", \n      \"confidence\": 0.0013656053900711037\n    }\n  ], \n  \"classifier_id\": \"f7ea68x308-nlc-144\", \n  \"top_class\": \"economia\"\n}\n"
                }
            ], 
            "source": "#Prueba 1\ntexto_1 = 'En pleno debate electoral, cuando los candidatos a la presidencia se la juegan apostando por subir o no subir los impuestos, lo que a los t\u00e9cnicos les parece populismo, el propio Fondo Monetario Internacional (FMI), que por dem\u00e1s, sorprendi\u00f3 con una baja en su proyecci\u00f3n de crecimiento de la econom\u00eda en el 2018 (de 3 a 2,7 %) y recomend\u00f3 seguir recortando las tasas de inter\u00e9s del Emisor, tambi\u00e9n le dijo al ministro de Hacienda, Mauricio C\u00e1rdenas, que no ve la necesidad de una nueva reforma tributaria en el pa\u00eds.'\nprueba_1 = natural_language_classifier.classify(classifier_id, texto_1)\nprint(json.dumps(prueba_1, indent=2))"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{\n  \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/f7ea68x308-nlc-144\", \n  \"text\": \"Aunque la ventaja es para el Real Madrid, tanto este equipo como el Paris Saint-Germain, que llega 3-1 abajo en la serie de octavos de final, llegan al Parque de los Pr\\u00edncipes a jugarse la temporada, nada menos. M\\u00e1s que el paso a la siguiente fase, no solo se juegan su presente, sino tambi\\u00e9n su futuro. A las 2:45 de la tarde comienza a rodar la bola. Transmite ESPN 2.\", \n  \"classes\": [\n    {\n      \"class_name\": \"deportes\", \n      \"confidence\": 0.9961117090763454\n    }, \n    {\n      \"class_name\": \"politica\", \n      \"confidence\": 0.0024911271985377436\n    }, \n    {\n      \"class_name\": \"economia\", \n      \"confidence\": 0.001397163725116979\n    }\n  ], \n  \"classifier_id\": \"f7ea68x308-nlc-144\", \n  \"top_class\": \"deportes\"\n}\n"
                }
            ], 
            "source": "#Prueba 2\ntexto_2 = 'Aunque la ventaja es para el Real Madrid, tanto este equipo como el Paris Saint-Germain, que llega 3-1 abajo en la serie de octavos de final, llegan al Parque de los Pr\u00edncipes a jugarse la temporada, nada menos. M\u00e1s que el paso a la siguiente fase, no solo se juegan su presente, sino tambi\u00e9n su futuro. A las 2:45 de la tarde comienza a rodar la bola. Transmite ESPN 2.'\nclasses = natural_language_classifier.classify(classifier_id, texto_2)\nprint(json.dumps(classes, indent=2))"
        }, 
        {
            "source": "#### Veamos cu\u00e1l fue el desempe\u00f1o del modelo sobre los datos de _test_", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Lo primero es obtener una predicci\u00f3n para cada una de las instancias que guardamos en nuestro test set", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "news_test = news_test.reset_index(drop= True)"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "cat_predicted = []\nfor i in range(0,len(news_test)):\n    #Extract the text to use in the API call\n    text = news_test.loc[i,0]\n    \n    #Get the result for the classification\n    class_result = natural_language_classifier.classify(classifier_id, text)\n    predicted_class_i = class_result['top_class']\n    cat_predicted.append(predicted_class_i)"
        }, 
        {
            "source": "Con esto ya podemos evaluar que tan preciso es nuestro modelo de evaluar noticias a nivel de oraciones", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 35, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.7882470451140337"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "accuracy_score(cat_test, cat_predicted)"
        }, 
        {
            "source": "Veamos espec\u00edficamente que tanto se equivoca en cada clase de categor\u00eda", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 36, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[2513,  164,  100,    0],\n       [ 107, 6173,   29,    0],\n       [ 169,   96,  784,    0],\n       [1353,  457,   69,    0]])"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "confusion_matrix(cat_test, cat_predicted, labels=['economia','deportes','politica', 'tecnologia'])"
        }, 
        {
            "source": "## Juguemos con el clasificador", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{\n  \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/f7ea68x308-nlc-144\", \n  \"text\": \"El presidente Florentino P\\u00e9rez manifest\\u00f3 que no se dejar\\u00e1 presionar por la oposici\\u00f3n al interior del Madrid\", \n  \"classes\": [\n    {\n      \"class_name\": \"politica\", \n      \"confidence\": 0.8393619964696493\n    }, \n    {\n      \"class_name\": \"deportes\", \n      \"confidence\": 0.14918065313906154\n    }, \n    {\n      \"class_name\": \"economia\", \n      \"confidence\": 0.01145735039128919\n    }\n  ], \n  \"classifier_id\": \"f7ea68x308-nlc-144\", \n  \"top_class\": \"politica\"\n}\n"
                }
            ], 
            "source": "#Prueba 2\ntexto_2 = 'El presidente Florentino P\u00e9rez manifest\u00f3 que no se dejar\u00e1 presionar por la oposici\u00f3n al interior del Madrid'\nclasses = natural_language_classifier.classify(classifier_id, texto_2)\nprint(json.dumps(classes, indent=2))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}